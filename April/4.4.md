**Subspace**
A subset of elements $S$ is called a subspace of a vector space $(V, F, +, \cdot),$ if and only if
1. $\vec 0 \in S$
2. $u, v\in S\implies u+v\in S$
3. $c\in F, u \in S\implies c\cdot u \in S$

**Thm**
Let $\{u_1, u_2,\ldots, u_k\}$ be a subset of the vectors in $\mathbb R^n.$ 
Then, the span of $\{u_1, u_2,\ldots, u_k\}$ is a subspace of $\mathbb R ^n.$
**Proof**
Let us $S$ be the span of $\{u_1, u_2,\ldots, u_k\}$ in $\mathbb R^n$
then, $$S = \{\vec v \in \mathbb R^n: \vec v = c_1\cdot u_1 +c_2\cdot u_2 +\ldots +c_k\cdot u_k,\space\space c_1, c_2,\ldots, c_k \in \mathbb R\}$$
we know that $0 \in \mathbb R$
so, take $\vec v = \sum_{i=0}^k 0\cdot u_i = \vec 0$
$\implies \vec v =\vec 0 \in S$

.
.
.


---

**Definition**
Consider an $m\times n$ matrix $A$ over $\mathbb R.$
The *row-space* of $A$ is the subspace of $\mathbb R^n$ spanned by the row vectors of $A$ and designated by $[row(A)].$
The *column-space* of $A$ is the subspace of $\mathbb R^m$ spanned by the column vectors designated by $[col(A)].$

To show that a particular vector $\vec b\in[col(A)],$ we need to be able to express it as a linear combination of the column vectors of $A.$

Consider the following matrix $A = \begin{bmatrix} 1 & -1 \\ 0 & 1 \\ 3 & -3 \end{bmatrix}.$ 
1. Does $\vec b = \begin{bmatrix} 1\\2\\3 \end{bmatrix} \in [col(A)]?$
2. Does $\vec w = \begin{bmatrix} 4 & 5 \end{bmatrix} \in [row(A)]?$


1. $\vec b = 3\cdot c_1 + 2 \cdot c_2$ and thus $\vec b \in [col(A)]$
2. $\vec w = 4\cdot r_1 + 9 \cdot r_2$ and thus $\vec w \in [row(A)]$

**Image : Rank**
The image of $T$ is the range of this function given  by $T(V)\subseteq W$.
The image of $T$ is a subspace in itself.
For every finite dimensional $V,$ $im(T)$ is also finite dimensional.
The dimension of $im(T)$ is called *rank*

**Null Space : Nullity**
The null space of $T$ is given by $$ker(T) = \{\vec v \in V : T(\vec v) = \vec0_w\}$$The Null Space of $T$ is a subspace in itself.
For every finite dimensional $V,$ $ker(T)$ is also finite dimensional.
The dimension of $ker(T)$ is called *nullity*.
Null space is also known as the *kernel* of $T$.


in conclusion:

$im(T) = \{\vec  w \in W : \exists \vec v \in V , T(\vec v) = \vec w\}$
$ker(T) = \{\vec  v \in V : T(\vec v) = \vec 0_w\}$
$rank = dim(im(T))$
$nullity = dim(ker(T))$

**Rank-Nullity Theorem**
$dim(V) = rank + nullity = dim(im(T))+ dim(ker(T))$

**Theorem**
Consider an $m\times n$ matrix $A$ over $\mathbb R.$ 
The null space of $A$ is a subspace of $\mathbb R^n$ which consists of the set of the homogenous linear system $AX = \vec 0.$


**Thm**
Consider an $n\times n$ matrix $A$ over $\mathbb R.$ 
The following are equivalent.
1. $A$ is invertible.
2. $AX=\vec0$ has only the trivial solution.
3. $AX = B, B\neq \vec 0, B\in \mathbb R^n$ has a unique non-trivial solutions.
4. The row reduced echelon form of $A$ is $\mathbb I _n.$ 
5. $A$ is a product of elementary matrices.
6. $rank(A) = n$
7. $nullity (A) = 0$
8. The column vectors of $A$ are linearly independent.
9. The column vectors of $A$ span $\mathbb R^n.$
10. The column vectors of $A$ are a basis for $\mathbb R^n.$
11. The row vectors of $A$ are linearly independent.
12. The row vectors of $A$ span $\mathbb R^n.$
13. The row vectors of $A$ are a basis for $\mathbb R^n.$



---

We also have that if the basis of $V$ is some $\{\vec\alpha_i\}^n_{i=1}\subset V$ and $dim(V) = n$
$T(\vec \alpha_i) = \vec \beta_i \in W$ 
and each $\vec \alpha = \sum_i c_i\vec\alpha_i$ 
$\vec\beta = T(\vec \alpha) = \sum_i c_i T(\vec\alpha_i) = \sum_i c_i\vec\beta_i$ 

As in, the rank will always be less than or equal to the dimension of $V$ because the dimension of $V$ is the number of vectors in the basis $B_v$ of $V$ which has some $n$ $\vec\beta_i$ vectors mapped in the image. Thus, each vector in the image can be represented by some linear combin
ation of the vectors $\{\vec\beta_i\}^n_{i=1}.$ So, the basis of the image of $V$ will be at most of dimension $n$ because there exists a set of $n$ linearly independent vectors  $,\{\vec\beta_i\}^n_{i=1},$ which span $im(T).$  

Consider some $F^n, F^m.$ Let us represent each vector (each n-tuple) in these spaces as a column matrix  and .
Thus, there exists some $A_{m\times n}$ matrix such that $$A_{m\times n} X_{n\times 1} = Y_{m\times 1}$$Thus, for every linear transformation, there exists a matrix representation, the rank of which is given by the number of linearly independent rows (or columns).
