
### Bases and Dimensions

**Definition**
Let $V$ be a vector space. A subset $S\subset V$ is linearly dependent if 
1. The set should contain distinct vectors. $$\overline \alpha_1, \overline \alpha_2,\ldots, \overline \alpha_n \in S$$
2. There $n$ scalars, such that at least one $c_i\neq0$ $$\exists c_1, c_2,\ldots, c_n\in F,$$ <p style="text-align:center;">such that </p> $$ \exists 1\leq i \leq n, c_i\neq0$$
3. $$c_1\overline\alpha_1+c_2\overline\alpha_2\ldots+c_n\overline\alpha_n = \overline0$$

**Corollaries**
1. If $S$ is not linearly dependent, it is linearly independent. 
2. If $S$ is linearly dependent, then every superset of $S$ is also linearly dependent. $$\forall $$
3. If $S$ is linearly independent, then every subset of $S$ is also linearly independent. $$$$If and only if every finite subset of a set $S$ is linearly independent, then $S$ is linearly independent.
4. If $S$ is a subspace of $V,$ then it must be linearly dependent.

**Definition**
Let $V$ be a vector space. A subset of vectors$,S,$ is called a *basis* of $S,$ if and only if
1. $S$ is linearly independent.
2. $S$ spans $V$

Recollecting the definition of span,
**Definition**
The smallest subspace of $V$ which contains a given *subset* of $V,$ $S\subset V, \overline0\in S,$ is the intersection$,W,$  of all of the subspaces of $V$ which contain $S$. This smallest subspace $W$ is known as the *span* of $S$ in $V$ or the subspace *spanned* by $S$ in $V$.

If $S$ is finite, i.e., $\exists n\in \mathbb N$ such that $S=\{\overline\alpha_1,\overline\alpha_2, \ldots \overline\alpha_n\}$ then $W$ is spanned by $\overline\alpha_1,\overline\alpha_2, \ldots \overline\alpha_n$

There can be multiple bases.
Consider the following 
$(F^3,F,+,.)$   $F\subset \mathbb  C$
$S=\{\overline\alpha_1,\overline\alpha_2,\overline\alpha_3,\overline\alpha_4\}$
$\overline\alpha_1 = (3,0,-3)$
$\overline\alpha_1 = (-1,1,2)$
$\overline\alpha_1 = (4,2,-1)$
$\overline\alpha_1 = (2,1,1)$


$c_1\overline\alpha_1+c_2\overline\alpha_2\ldots+c_3\overline\alpha_3\ldots+ c_4\overline\alpha_4 = \overline0$
$c_1 =2, c_2 =2, c_3 = -1, c_4 = 0$

This is linearly dependent.


**Definition**
For any $F^n$ over $F,$ consider the $\{\overline\epsilon_1,\overline\epsilon_2,\overline\epsilon_3, \ldots, \overline\epsilon_n\}$
$\overline\epsilon_1 = (1, 0, 0, \ldots 0)$
$\overline\epsilon_2 = (0, 1, 0, \ldots 0)$
$\overline\epsilon_2 = (0, 0, 1, \ldots 0)$
.
.
$\overline\epsilon_n = (0, 0, 0, \ldots 1)$

This is known as the *standard basis*.
It is an ordered and orthonormal basis.


In the form of a matrix $P_{n\times n},$
$P=\begin{bmatrix} P_1&P_2&\ldots&P_n\end{bmatrix}$ (which are the columns of $P$)
$P_1, P_2, \ldots, P_n$ form bases for the vector space of $n\times1$ matrices.

that is, any column matrix $Y$ can be written as
$x_1P_1+x_2P_2\ldots+x_nP_n = Y$

$PX=0\iff X=0$ 
$Y_{n\times 1}=PX$ 
$X=P^{-1}Y$


**Thm** 
Let $V$ be a vector space. Let $B$ be a linearly independent subset of vectors $B=\{\overline \beta_1, \overline\beta_2, \ldots,\overline\beta_n\}$ such that $B$ spans $V$. (I.e., let $B$ be a basis of $V$.) Then no linearly independent subset of $V$ can have more than $n$ elements.

I.e., every basis has $n$ elements, and no other linearly independent subset has more elements than the basis.

**Proof**
We have to show that any set $S$ of arbitrary vectors with $m>n$ elements is linearly dependent. 
Let $S=\{\overline\alpha_1,\overline\alpha_2, \ldots, \overline\alpha_m\}$ be a subset of $V$ such that $m>n$ 
We have that $B=\{\overline \beta_1, \overline\beta_2, \ldots,\overline\beta_n\}$ spans $V$ and $B$ is linearly independent.

$\forall j,$
$$ \overline\alpha_j = \sum_i A_{ij}\overline\beta_i$$

$$\overline 0 = \sum_j x_j\overline\alpha_j=\sum_j (x_j\sum_i A_{ij}\overline\beta_i)=\sum_j \sum_i(x_j A_{ij}\overline\beta_i)$$
but we have that
$$\sum_ix_j A_{ij}=0$$
because number of rows is less than equal to number of columns, so there is at least one solution to this system such that $\sum_ix_j A_{ij}=0$.

**Definition**
The number of elements in a basis of a finite vector space is known as the *dimension* of the vector space.

**Corollaries**
Let $n$ be the dimension of a vector space $V$
1. $|S|>n\implies S$ is linearly dependent.
2. $|S|<n\implies S$ does not span $V$.
3. $|S|=n\implies S$ is a basis of $V$.

**Lemma**
Let $S$ be a linearly independent subset of $V$ and $\overline \beta\in W$ where $W$ is a subspace not spanned by $\overline\beta$.

Then $S\cup\overline\beta$ is also linearly independent, because
$$c_1\overline\alpha_1+c_2\overline\alpha_2\ldots+c_n\overline\alpha_n + b\overline\beta = \overline0$$ if 
$\overline\beta = -\frac{c_1}{b}\overline\alpha_1-\frac{c_2}{b}\overline\alpha_2\ldots-\frac{c_n}{b}\overline\alpha_n\implies\overline\beta = \overline0$ 



**Thm** (Exercise)
If $W$ is a subspace of a vector space $V$ of finite dimension, then every independent subset of $W$ is finite and is part of a (finite) basis for $W$.

**Corollary**
1. Every basis of $V$ have the same number of elements, which is the dimension of $V$.
2. $W\subset V\implies$ dim$(W)<$dim$(V)$





Consider a matrix $A_{n\times n}$

For an invertible matrix $A_{n\times n}$
1. Each row matrix is linearly independent.
2. Each column matrix is linearly independent.

If for a matrix $A_{n\times n}$ each row is linearly independent or each column is linearly independent, then it is an invertible matrix.


If $W_1$ and $W_2$ are two finite-dimensional subspaces of $V,$ then $W_1+W_2$ is also a finite-dimensional subspace, and

dim$(W_1)+$dim$(W_2) =$ dim $(W_1\cap W_2)+$dim$(W_1+W_2)$ 

