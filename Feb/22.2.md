
You can write any linear transformation as a matrix notation as long as you fix an ordered basis for both.
$A[\vec\alpha]_{B_V} = [\vec\beta]_{B_W}$
where the matrix $A$ is a matrix representation of some linear transformation, and $[\vec\alpha]_{B_V}$ is the column matrix for some ordered basis of $V$ and $[\vec\beta]_{B_W}$ is some ordered basis of $W$.

**Lemma** (Proof: Exercise)
Let $V$ be a vector space over $F$.
Let $U,T,T_1,T_2\in\mathcal L(V,V), c\in F.$
We have then the following.
1. $\mathbb1U = U\mathbb1 = U$
2. $U(T_1+T_2)=UT_1+UT_2$
3. $(T_1+T_2)U=T_1U + T_2U$
4. $c(UT) = (cU)T = U(cT)$

Where  $\mathbb1(\vec v) =\vec v,\forall \vec v \in V$


**Inverse of Linear Transformations**

A linear transformation is invertible if and only if:
1. $T$ is one-one
	$T\vec\alpha = T\vec\beta \implies \vec\alpha = \vec\beta$
2. $T$ is onto
	$im(T)=W$

**Definition**
**Invertible Linear Transformation**
A linear transformation $T:V\rightarrow W$ is invertible if and only if $\exists U:W\rightarrow V$ such that 
1. $UT =  \mathbb 1_V : V\rightarrow V$
2. $TU = \mathbb 1_W : W\rightarrow W$
In this case, $U$ is called the inverse of $T$ and it is represented by $T^{-1}$

So, 
1. $T^{-1}T =  \mathbb 1_V : V\rightarrow V$
2. $TT^{-1} = \mathbb 1_W : W\rightarrow W$


**Thm**
Let $V$ and $W$ be two finite dimensional vector spaces over the field $F$. 
Let $T:V\rightarrow W$ be an invertible linear transformation.

Then, 
$T^{-1}$ is also an invertible linear transformation.

**Proof**
$\forall \vec\alpha, \vec\beta\in V, \forall c \in F$
$T(c\vec\alpha+\vec\beta)$
$=cT\vec\alpha+T\vec\beta$
$=T^{-1}(T(c\vec\alpha+\vec\beta))$
$=T^{-1}(T(c\vec\alpha))+T^{-1}(T(\vec\beta))$
$=c\vec\alpha + \vec\beta$

**Thm**
$f$ is invertible and $g$ is invertible, and $f\circ g$ exists $\iff$ $(f\circ g)^{-1}$ exists and $(f\circ g)^{-1} = g^{-1}\circ f^{-1}$  

i.e.

For every invertible $T:V\rightarrow W$ and $U:W\rightarrow Z$ 
We have $(UT)^{-1} = T^{-1}U^{-1}$

**Definition**
Non-Singular Matrix
$T(\vec\gamma)=\vec0_W\implies \vec\gamma=\vec0_V$ 
i.e. if $nullity(T) = 1$ then it is called a non singular matrix.
$T$ is one-one $\iff$ $T$ is non-singular
I.e., $T(\vec\alpha) = T(\vec\beta) \implies \vec\alpha = \vec\beta$ because
if $T(\vec\alpha - \vec\beta) =T(\vec\alpha) - T(\vec\beta),\space T(\vec\alpha - \vec\beta) = \vec0_W\implies\vec\alpha - \vec\beta = \vec0_V\implies \vec\alpha = \vec\beta$
Formally, a linear transformation $T:V\rightarrow W$ is non-singular $\iff$ for each linearly independent subset $S\subseteq V,$ $T(S)\subseteq W$ is also a linearly independent subset.
A linear transformation being onto and non-singular do not imply each other.

**Thm**
Let $V$ and $W$ be two finite dimensional vector spaces over $F$. 
Consider some linear transformation $T:V\rightarrow W.$ 
The following are equivalent:
1. $T$ is invertible.
2. $T$ is non-singular.
3. $T$ is onto ($range(T)=W$)
4. If $\{\vec\alpha_1,\vec\alpha_2, \ldots, \vec\alpha_n\}$ is a basis for $V,$ then $T\{(\vec\alpha_1, \ldots, T(\vec\alpha_n))\}$ is a basis for $W$.
5. There is some basis $\{\vec\alpha_1,\vec\alpha_2, \ldots, \vec\alpha_n\}$ of $V$ such that $T\{(\vec\alpha_1, \ldots, T(\vec\alpha_n))\}$ is a basis for $W$.