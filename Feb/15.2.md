
**Definition** 
**Linear Transformation**
Let $V$ and $W$ be two vector spaces over $F$. Then a map $T:V\rightarrow W$ is called a linear transformation if

$\forall \vec \alpha, \vec \beta \in V, \forall c\in F$ $$T(c\cdot\vec\alpha+\vec\beta) = c\cdot T(\vec\alpha) + T(\vec\beta)$$

Examples
1. $T(\vec\alpha) = \vec0_W$
2. if $V$ and $W$ are the set of all polynomials in one variable of degree $n$ and $n-1$ respectively, then $T(\vec\alpha) = \frac{d\vec\alpha}{dx}$ (differentiation) is a linear transformation.

**Remark**
$T(\vec0_V) = \vec0_W$
**Proof**
$T(\vec0_V) = T(1\cdot\vec0_V +\vec0_V) = 1\cdot T(\vec0_V) + T(\vec0_V)$
$T(\vec0_V) = T(\vec0_V) + T(\vec0_V)$ 
$T(\vec0_V) - T(\vec0_V)= T(\vec0_V)$
$\vec0_W =T(\vec0_V)$
$\implies T(\vec0_V) = \vec0_W$

**Remark**
$$T(\sum_{i=1}^nc_i\vec\alpha_i ) = \sum_{i=1}^nc_iT(\vec\alpha_i)$$
Thus, this function preserves the linear transformation in a subspace. Thus, we also have that


Thus, even the image of $T$ is also a subspace because
$c\cdot T(\vec\alpha) + T(\vec\beta) =  T(c\cdot\vec\alpha+\vec\beta) \in$ image$(T),$   $\forall c\in F,$  $\forall\space T(\vec\alpha), T(\vec\beta)\in W$ and $c\cdot\vec\alpha+\vec\beta\in V$ 
**Thm**
Let $V$ be a finite dimensional vector space over $F.$ 
Let there be an ordered basis $B$ for $V$ such that $B = \{\vec\alpha_1,\vec\alpha_2, \vec\alpha_3,\ldots, \vec\alpha_n\}$.
Let there be some $n$ vectors $\vec\beta_1,\vec\beta_2, \vec\beta_3,\ldots, \vec\beta_n \in W.$ 

Then, there is precisely one linear transformation $T:V\rightarrow W$ such that $\forall i\in \{1, 2, \ldots, n\}$ $$T(\vec\alpha_i) = \vec\beta_i$$
**Proof**
We need to begin by proving that
$\exists T:V\rightarrow W$ such that $T(\vec\alpha_i) = \vec\beta_i$  $\forall i$

We know that $\forall\vec\alpha\in V,$ $\exists$ an n-tuple such that
$$\vec\alpha = x_1\vec\alpha_1 +x_2\vec\alpha_2 +\ldots + x_n\vec\alpha_n$$ because $B$ is a basis of $V.$
Now, take some rule $T,$ such that
$$T(\vec\alpha) = x_1\vec\beta_1 +x_2\vec\beta_2 +\ldots + x_n\vec\beta_n$$
Now take another $\vec\gamma\in V$ such that$$\vec\gamma= y_1\vec\alpha_1  +y_2\vec\alpha_2 +\ldots + y_n\vec\alpha_n$$ thus, $$T(\vec\gamma)= y_1\vec\beta_1  +y_2\vec\beta_2 +\ldots + y_n\vec\beta_n$$
$$T(c\cdot\vec\alpha +\vec\gamma) = T((cx_1+y_1)\cdot\vec\alpha_1 + (cx_2+y_2)\cdot\vec\alpha_2 + \dots + (cx_n+y_n)\cdot\vec\alpha_n)$$ $$=(cx_1+y_1)\cdot\vec\beta_1 + (cx_2+y_2)\cdot\vec\beta_2 + \dots + (cx_n+y_n)\cdot\vec\beta_n$$
$$=(cx_1\cdot\vec\beta_1+y_1\cdot\vec\beta_1)+ (cx_2\cdot\vec\beta_2+y_2\cdot\vec\beta_2)\ldots + (cx_n\cdot\vec\beta_n+y_n\cdot\vec\beta_n)$$
$$=(cx_1\cdot\vec\beta_1+cx_2\cdot\vec\beta_2+\ldots + cx_n\cdot\vec\beta_n)+ (y_1\cdot\vec\beta_1+y_2\cdot\vec\beta_2+\ldots+y_n\cdot\vec\beta_n)$$
$$ = cT(\vec\alpha) + T(\vec\gamma)$$
And thus, we have that $T$ is a linear transformation. Now that we know that there exists at least one linear transformation, we must prove that there exists at most one transformation.

We have that
$T(\vec\alpha_i) = \vec\beta_i$   $\forall i$
Let us assume that there is one more linear transformation. 
$U(\vec\alpha_i) = \vec\beta_i$   $\forall i$

%% fill in here %%




**Definition**
**Null Space**
If $V$ and $W$ are two vector spaces over $F,$ the null space of $T:V\rightarrow W$ is the set of all vectors $\vec\alpha \in V$ such that $$T(\vec\alpha) = \vec0_W$$


